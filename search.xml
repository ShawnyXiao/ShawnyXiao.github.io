<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【数据挖掘比赛】企业经营退出风险预测]]></title>
    <url>%2F2018%2F02%2F07%2F%E3%80%90%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%E3%80%91%E4%BC%81%E4%B8%9A%E7%BB%8F%E8%90%A5%E9%80%80%E5%87%BA%E9%A3%8E%E9%99%A9%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[这是我的第一个数据挖掘比赛，CCF 大数据与计算智能大赛（BDCI）中的一题：企业经营退出风险预测。最终取得复赛 A 榜第 3，B 榜第 9 (Top 1.58%) 的成绩。 这个比赛 12 月中旬就结束了，硬是被我拖到现在才来总结，我这拖延症真的是……现在回忆起这个比赛，比赛时的那种郁闷感依然记忆犹新。我在复赛的第 5 天便达到了分数 6924，但之后一直无法提分，这种烦躁感当时给我带来了挺大的困扰（当然最后还是提升到了分数 6930）。等比赛结束之后，我回过头来看，其实当时我参赛的心态是不端正的，功利心太强，这样带来的问题就是比赛心态的爆炸，自己的眼界会被约束，提分方式的想象力也会被限制。最好的心态应该是抱着学习的心态参赛，只要能够学到一点点新的东西，就会感到惊喜。 另外一个想说的点是，我们团队在复赛 A 榜中排名第 3，但是切换 B 榜之后，便跌到第 9 了，这个现象直接导致我们团队没有进入决赛，因此我会在后文中谈一谈为什么会有这个现象。 我的另一位队友 JinjinLin 也开源了解决方案，详情请见 JinjinLin 的解决方案 项目源码：https://github.com/ShawnyXiao/2017-CCF-BDCI-AIJudge Why?CCF 举办的这次大赛中这么多比赛，为什么唯独选择这个呢？ 因为门槛低。我在参赛之前对所有的比赛有过大致的了解，其中比赛类型包括：自然语言处理（NLP）、计算机视觉（CV）和传统的数据挖掘比赛等等。作为一个第一次参赛的新人，我的重心不会放在需要一定的门槛的比赛，因此就排除了 NLP 和 CV 的比赛，再挑一个门槛最低的，那么目标就锁定了，于是我便将重心放在了企业经营退出风险预测这个比赛。 因为有师兄带（提供 baseline，指导尝试方向）。今年的 CCF 举办的大赛，我们实验室不少人参赛了，其中也包括不少往年拿过奖的师兄，他们有参赛经验。作为一只菜鸟，自然是希望有人能够给予少走弯路的建议。而师兄也建议新手参加这个方式相对简单的比赛作为入门。 为什么我想要说一下这个呢，因为我相信未来有很多的新人会尝试加入数据挖掘的阵营中，他们也会遇到相同的境遇，我希望能够将我当时的一些思考与选择作为他们的参考选项，以便于他们做出他们的最优选择。 代码框架第一次参赛，可以说连 Python 的语法都不熟悉，更何况 pandas 的各种操作。这时候师兄给的 baseline 就显得十分重要了。当中的各种基础操作，例如：文件读取、数据定义、分组聚集等等，对我来说都是新鲜的。其中最为关键的是传统的数据挖掘比赛中的代码框架。我们来看一下，这个极为经典的代码框架（非原始 baseline 框架，我做了一些修改）。 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 1. 导入库import numpy as npimport pandas as pd...# 2. 读取数据文件train = pd.read_csv('../data/input/train.csv')test = pd.read_csv('../data/input/evaluation_public.csv')...# 3. 定义特征构建函数def get_entbase_feature(df): ...def get_alter_feature(df): ......# 4. 调用函数，构建特征entbase_feat = get_entbase_feature(entbase)alter_feat = get_alter_feature(alter)...# 5. 拆分数据集的特征与标签dataset = pd.merge(entbase_feat, alter_feat, on='EID', how='left')...trainset = pd.merge(train, dataset, on='EID', how='left')testset = pd.merge(test, dataset, on='EID', how='left')train_feature = trainset.drop(['TARGET', 'ENDDATE'], axis=1)train_label = trainset.TARGET.valuestest_feature = testsettest_index = testset.EID.values# 6. 模型的交叉验证...iterations, best_score = xgb_cv(train_feature, train_label, params, config['folds'], config['rounds'])...# 7. 模型的训练与预测...model, pred = xgb_predict(train_feature, train_label, test_feature, iterations, params)...# 8. 结果文件的写出res = store_result(test_index, pred, 0.18, '1207-xgb-%f(r%d)' % (best_score, iterations)) 从上面给的样例代码中，我们可以观察到整个代码的框架如下： 导入库 读取数据文件 定义特征构建函数 调用函数，构建特征 拆分数据集的特征与标签 模型的交叉验证 模型的训练与预测 结果文件的写出 使用这样一个代码框架，能够十分清晰的知道整个数据挖掘的流程，这一点对于第一次参赛的信任是尤为重要的。另外当我们想要提分时，我们只需要在特定的部分做出相应的修改就能够达到目的。例如：我希望构建新的特征，来提升我的分数，那么这时只需要新增框架中的第 3 和第 4 部分即可。 数据预处理这个数据集中存在着不少的脏数据，这个阶段便是对这些脏数据进行处理，其中包括： 转化或者移除数据中存在的中文字符 针对性的空值填充 针对性地去除重复值 异常值的处理（这点我没有做） 特征我将特征分为 5 个部分，分别是基础特征、偏离值特征、交叉特征和想象力特征。 1. 基础特征基础特征是比赛中最容易想到的特征，其中包括： 保留字段。数据集中某些关键字段直接保留成特征，例如：uid、ZCZB、RGYEAR、INUM、ENUM 等 统计特征。以某几个字段作为分组字段，然后进行统计操作，统计操作包括：计数、求和、最小值、最大值、最小最大差值、均值、标准差、比例等 特定集合中的统计特征。先进行过滤，然后以某几个字段作为分组字段，然后进行统计操作。例如：统计近 1、2、5 年内的修改数额的最小值、最大值和均值等 2. 偏离值特征偏离值特征指单个个体与分组之间的偏离距离。以下的代码所生成的特征便是这一类特征： 1234567891011121314151617181920dataset['MPNUM_CLASS'] = dataset['INUM'].apply(lambda x : x if x &lt;= 4 else 5)dataset['FSTINUM_CLASS'] = dataset['FSTINUM'].apply(lambda x : x if x &lt;= 6 else 7)dataset.fillna(value=&#123;'alt_count': 0, 'rig_count': 0&#125;, inplace=True)for column in ['MPNUM', 'INUM', 'FINZB', 'FSTINUM', 'TZINUM', 'ENUM', 'ZCZB', 'allnum', 'RGYEAR', 'alt_count', 'rig_count']: groupby_list = [['HY'], ['ETYPE'], ['HY', 'ETYPE'], ['HY', 'PROV'], ['ETYPE', 'PROV'], ['MPNUM_CLASS'], ['FSTINUM_CLASS']] for groupby in groupby_list: if 'MPNUM_CLASS' in groupby and column == 'MPNUM': continue if 'FSTINUM_CLASS' in groupby and column == 'FSTINUM': continue groupby_keylist = [] for key in groupby: groupby_keylist.append(dataset[key]) tmp = dataset[column].groupby(groupby_keylist).agg([sum, min, max, np.mean]).reset_index() tmp = pd.merge(dataset, tmp, on=groupby, how='left') dataset['ent_' + column.lower() + '-mean_gb_' + '_'.join(groupby).lower()] = dataset[column] - tmp['mean'] dataset['ent_' + column.lower() + '-min_gb_' + '_'.join(groupby).lower()] = dataset[column] - tmp['min'] dataset['ent_' + column.lower() + '-max_gb_' + '_'.join(groupby).lower()] = dataset[column] - tmp['max'] dataset['ent_' + column.lower() + '/sum_gb_' + '_'.join(groupby).lower()] = dataset[column] / tmp['sum']dataset.drop(['MPNUM_CLASS', 'FSTINUM_CLASS'], axis=1, inplace=True) 这段代码的意思是： 首先，根据分组字段对数据集进行分组 然后计算每个个体与分组的均值、最小值、最大值和求和值之间的偏离距离 这类特征对于这个比赛十分有效，是我分数大幅上升的一个原因。 3. 交叉特征交叉特征指不单单从一个角度去构建特征，而从多个角度构建够特征，或者说将特征之间相互作用后生成新的特征。这类特征包括： 加减乘除特征。将特征与特征做加减乘除操作，也就是所谓的暴力出奇迹。例如：MPNUM+INUM、FINZB/ZCZB 等 独热交叉特征。将一些特征做独热编码后，然后乘以某个特征。例如：将 HY 做独热编码后，乘以 ZCZB、RGYEAR 等 多项式交叉特征。对特征做多项式组合。例如：MPNUM^2+INUM 等（我没有做这类交叉特征） 交叉特征的效果也十分明显，能显著的提升分数，其中独热交叉特征在这个比赛中最为有效。 4. 想象力特征想象力特征这个词是我自己构造的，指的是根据实际的业务场景，思考其中可能存在的一些隐晦的特征。例如：投资表中，就可以构建一个投资网络，然后基于这个网络提取相关的特征。这个思路来自我的师兄 @Kaho，这也是我赛后才了解到的特征构造方式，十分新颖。 模型模型部分包括：单模型的提分与多模型融合。 首先，谈谈单模型的提分。在这个比赛中，根据师兄的建议，我选择了 XGBoost，使用它的原因在于： 树模型有较强的可解释性，往往简单且高效 树模型对于异常值有较强的鲁棒性 树模型对特征处理的要求比较低，不需要对特征进行归一化与空值填充 其次，是多模型融合。这部分是我的另一位队友做的，因此我没有过多的尝试多模型融合。在这个比赛中，我们团队的融合效果不是太好，加权融合之后分数仅提升 1 至 2 个千。 踩过的坑新人入赛不踩坑是不可能的，比赛中我是踩了无数个坑，其中比较有意思的，比较隐晦的有这么几个： 不要带着刻板印象去筛选特征，换句话说，你不要觉得其他比赛没用的特征对于这个比赛同样没用。在这个比赛中，ID 特征是一个强特征，我刚开始就带着刻板印象把它删了，导致 3 个千分点的劣势，发现这个问题也耗费了不少时间 在对 dataframe 排序之后一定要 调用 reset_index(drop=True)，不然之后对这个 dataframe 的各种操作的是误操作。这个坑同样耗费了我不少的精力 不要太早就开始模型调参，模型调参只能带来极少的提升，在你的分数没有达到一定竞争力的时候，调参带来的收益是极少的，因此在调参这个举动的价值在比赛早期是较低的 复赛开始后，初赛数据别果断抛弃，应该试一试效果，辩证式的采纳 没尝试的点 没尝试融合大法。因为团队中有队员负责融合，所以在比赛中我没有尝试融合大法，这点比较可惜。另外我们团队的融合策略是 blending（加权融合），还可以尝试的策略包括：stacking、bagging 等 没尝试使用初赛的数据。这点输在新人没经验，根本没有意识到可以使用初赛的数据 未进决赛的原因分析我们团队在复赛 A 榜中排名第 3，但是切换 B 榜之后，便跌到第 9 了，这个现象直接导致我们团队没有进入决赛，在赛后我进行了认真的分析与思考，并且与他人探讨，大致总结了几点原因： 未使用初赛提供的数据。由于我们是新人队伍，使用初赛数据这个套路我们完全没有考虑到，这样就使得其他既使用了复赛数据也使用了初赛数据的队伍能够占据较大优势 我们加权融合的依据是 A 榜的线上分数，这样有极大记录过拟合 A 榜，更好的做法应该是综合考虑线下分数与 A 榜线上分数，以避免出现过拟合现象 我们队伍都是来自一个实验室，和队之后，队伍内部有比较多的交流，这可能导致我们的特征相似度比较大，这样融合之后的效果不会特别好，因此我们融合值提升了 1 至 2 个千分点 嘿！感谢以下朋友，他们向我输送了一些新的观点： @/微笑/:)/wx，他提出：我们团队来自一个实验室，特征可能比较相似，导致融合效果不好 如果您有任何的想法，例如：发现某处有 bug、觉得我对某个方法的讲解不正确或者不透彻、有更加有创意的见解，欢迎随时发 issue 或者 pull request 或者直接与我讨论！另外您若能 star 或者 fork 这个项目以激励刚刚踏入数据挖掘的我，我会感激不尽~]]></content>
      <categories>
        <category>数据挖掘比赛</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【数据挖掘比赛】让AI当法官]]></title>
    <url>%2F2018%2F01%2F17%2F%E3%80%90%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%E3%80%91%E8%AE%A9AI%E5%BD%93%E6%B3%95%E5%AE%98%2F</url>
    <content type="text"><![CDATA[这是我近期参加的一个数据挖掘比赛，CCF 大数据与计算智能大赛（BDCI）中的一题：让 AI 当法官。但是由于时间冲突与一些个人原因，我只参与并完成了初赛任务（罚金类别预测），并未完成复赛任务（法律条款预测）。在初赛成绩中，取得 A 榜第 5、B 榜第 7 (Top 1.68%) 的成绩（这个成绩实际上只用了 if-idf 特征和 Word2Vec 特征，该项目在初赛结束后做了不少尝试和改进，效果应该会更优于初赛）。 该项目是我的第一个有关文本分类的项目，所以在做这题之前，我没有任何自然语言处理（NLP）的知识积累。因此，通过参与这个比赛，我的初衷是希望学到一些自然语言处理的基础知识，所以名次对于我来说没有那么重要了。 作为一个零基础的选手，自己从零构建整个项目是非常困难的。于是，我搜索了往年的有关文本分类的比赛，挑出了一些对于我个人而言，比较容易阅读的一些方案与源码。我选择了 2016 年的比赛：大数据精准营销中搜狗用户画像挖掘，并找到了一等奖与二等奖的方案与源码，进行了详细的阅读，阅读源码的过程是很痛苦的，因为太多的这是什么那是什么这也不懂哪也不懂，但是正是经历过这样一个过程才能真正学到一些知识并完成一些实践。最终，依靠前辈们的方案与源码，零基础的我成功搭建了一个 baseline 项目并在其上做出自己的一些思考与优化。 项目源码：https://github.com/ShawnyXiao/2017-CCF-BDCI-AIJudge 方案1 数据预处理数据预处理包括分词和去除停用词，其达成的效果大致如下。 公诉机关霍邱县人民检察院。被告人许某甲，男，1975年9月20日生。2012年12月17日因涉嫌危险驾驶罪由霍邱县公安局取保候审。2013年3月4日经本院决定取保候审。霍邱县人民检察院以霍检刑诉（2013）42号起诉书指控被告人许某甲犯危险驾驶罪，于2013年2月27日向本院提起公诉。本院依法适用简易程序，实行独任审判，于2013年3月4日公开开庭审理了本案。霍邱县人民检察院检察员胡涛、被告人许某甲到庭参加诉讼。现已审理终结。霍邱县人民检察院指控：2012年12月2日19时30分左右，被告人许某甲酒后驾驶二轮摩托车沿霍寿路由南向北行驶至霍寿路与公园路交叉口时，与路边行人相撞，被公安民警查获。经六安市疾病预防控制中心鉴定，许某甲血液中乙醇含量为169.64mg／100ml。上述事实，被告人在开庭审理过程中亦无异议，并有被害人杨正响的陈述，证人李某甲的证言，《六安市疾病预防控制中心检验报告》六安市疾控交检字（2012）第155号，霍邱县公安局交通管理大队呼吸式酒精检测结果单，抽取当事人血样登记表，驾驶人信息查询结果单，道路交通事故赔偿调解协议书、经济赔偿凭证、谅解书，被告人的户籍信息等证据证明，足以认定。 会被转化为： 公诉 机关 霍邱县 人民检察院 被告人 许某 甲 男 1975 年 月 20 日生 2012 年 12 月 17 日 因涉嫌 危险 驾驶 罪 霍邱县 公安局 取保候审 2013 年 月 日经 本院 取保候审 霍邱县 人民检察院 以霍检 刑诉 2013 42 号 起诉书 指控 被告人 许某 甲犯 危险 驾驶 罪 2013 年 月 27 日向 本院 提起公诉 本院 依法 简易程序 实行 独任 审判 2013 年 月 日 公开 开庭审理 本案 霍邱县 人民检察院 检察员 胡涛 被告人 许某 甲 到庭 参加 诉讼 现已 审理 终结 霍邱县 人民检察院 指控 2012 年 12 月 日 19 时 30 分 被告人 许某 甲 酒后 驾驶 二轮 摩托车 沿霍寿路 由南向北 行驶 霍寿路 公园路 交叉口 时 路边 行人 相撞 公安民警 查获 六安市 疾病 预防 控制中心 鉴定 许某 甲 血液 中 乙醇 含量 169.64 mg 100ml 上述事实 被告人 开庭审理 过程 中 无异议 被害人 杨正响 陈述 证人 李某 甲 证言 六安市 疾病 预防 控制中心 检验 报告 六安市 疾控交 检字 2012 155 号 霍邱县 公安局 交通管理 大队 呼吸 式 酒精 检测 抽取 当事人 血样 登记表 驾驶 信息 查询 道路 交通事故 赔偿 调解 协议书 经济 赔偿 凭证 谅解 书 被告人 户籍 信息 证据 证明 足以认定 接下来，我会对这两个子处理进行介绍，并做出一些思考：在这些处理之上还能做些什么。 1.1 分词为了快速构建项目，我直接采取了比较热门的分词方案：结巴分词。 实际上，在此基础上还可以做的事情有很多（虽然我没有做），例如： 采用多种分词方案（例如：NLPIR、THULC 等），实现分词 使用某种模型（例如：贝叶斯模型等）比较这多种分词方案的效果 对分词后的语料库进行统计分析，归纳改语料库的特点（例如：字典长度为多少；低频词多吗；该不该过滤掉某些词等等） 1.2 去除停用词中文中有非常多的停用词，这些停用词对于我们的文本分类任务是无用的。因此，我采取的措施是：直接去除。 有些任务对于一些停用词是敏感的。在这个阶段，还可以做的有： 对停用词的进行分析，猜想某些停用词是有用的并验证猜想（个人感觉这个比赛的停用词是无用的） 2 特征构建我从 4 个方面对文本进行特征构建，分别是：基于 tf-idf 的特征、基于 Doc2Vec 的特征、基于 Word2Vec 的特征和统计特征。接下来，我会从这 4 个方面分别介绍。 2.1 基于 tf-idf 的特征tf-idf（英语：term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。tf-idf 是一种统计方法，用以评估一字词对于一个文档集或一个语料库中的其中一份文档的重要程度。字词的重要性随着它在文档中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。 我直接使用了 TfidfVectorizer 提取了语料库的 tf-idf 特征。但是 tf-idf 特征具有多维稀疏的特点。对于这类特征，直接扔给树模型的话不仅慢而且效果还差，因此比较流行的做法是做一层 stacking。我挑选了 LogisticRegression、BernoulliNB、MultinomialNB 和 LinearSVC 作为基模型分别对 tf-idf 特征进行训练，并构建下一层模型需要的特征，因此理论上能够产生 4*8 的特征列数。但最终根据实验结果，我移除掉了 LinearSVC 模型，因此只留下了 3*8 列的特征。 这里，还可以做的事情有： 选取更多的模型对 tf-idf 特征进行训练，并比较效果，选取实验效果最好的几个进行 stacking 2.2 基于 Doc2Vec 的特征我使用 Doc2Vec 方法，将文档直接表示成一个固定长度的向量。根据训练文档向量的网络结构的不同，可以分为 Distributed Memory（DM）与 Distributed Bag of Words（DBOW）两种模型。其中 DM 模型不仅考虑了词的上下文语义特征，还考虑到了词序信息。DBOW 模型则忽略了上下文词序信息，而专注于文档中的各个词的语义信息。我同时采用了 DBOW 和 DM 这两种模型构建文档向量，希望能够保留文档中完整的信息。 对于 Doc2Vec 模型，我选取的维数是 300。在训练文档向量的过程中，我发现增量训练似乎可以提升文本分类的精度，因此我在训练过程中增加了训练的次数，DBOW 模型的训练次数为 5，DM 模型的训练次数为 10。 同样的，我对于这两类文档向量分别做了一层 stacking，使用了一个简易的神经网络模型，只有一层 300 维的隐含层，进行训练并构建下一层模型需要的特征。 这个地方，还可以做得事情有： 比较 DBOW 和 DM 模型的效果，确定是否选用其中一种或者选用两种 选用多种模型（例如：LR、NN、KNN 等）对这两类文档向量进行效果对比，选取其中最好的几种模型进行 stacking 调优超参，包括 Doc2Vec 模型维数、增量训练次数等 2.3 基于 Word2Vec 的特征我使用了 Word2Vec 方法，将词语直接表示成一个固定长度的向量。对于 Word2Vec 模型，我选取的维数为 300，并将频数低于 5 的词语过滤掉。 那么对于一个文档来说，这些针对词语的向量要怎么处理呢？我选择了两种方式： 属于同一个文档的词向量，直接相加 属于同一个文档的词向量，加权平均（相加后的结果向量再除以文档的词数目） 由于一些个人原因，我只使用了第一种方式产生的特征向量。 实际上，这里可以做的事情还有： 对这两种方式生成的特征向量设计实验（例如：只选用其中一种特征向量，同时选用两种特征向量），进行效果比较，选取效果最好的方式 对于词向量维数和过滤的词频进行实验，选择最优的超参 设计步长参数，选取多个维数所产生的词向量（例如：选择步长为 50，维数从 100 到 500。那么可以产生 100、150、200、……、500 维数的词向量） 2.4 统计特征该文本分类任务是预测案件金额类别，因此案件文本中出现的金额是重要的。于是，我使用正则表达式匹配出案件文本中出现的所有金额，然后对同一个案件中出现的所有金额进行统计，包括：求和、最小值、最大值、最大最小差值、平均值、标准差。 这里还可以做的事情有： 统计案件文本的词的数目 利用案件中的一些关键词做特征，例如：酒驾、毒品等 案件文本中出现的日期 案件文本中出现的地点 3 模型 上图给出了本次项目的模型结构。我采用了模型融合中 stacking 的思想，使用两层的模型结构。第一层使用传统的机器学习模型 LogisticRegression、BernoulliNB 和 MultinomialNB，来训练 tf-idf 特征，从而学习案件文本中的用词特点；其次还使用神经网络模型来训练 Doc2Vec-DBOW 和 Doc2Vec-DM 生成的文档向量特征，从而学习案件文本中的词语的语义关联信息。第二层使用 XGBoost 模型，训练 Word2Vec、统计特征和第一层模型传来的概率特征，从而更深入的学习案件文本与金额类别之间的联系。采用 stacking 的模型融合思想，可以进一步的提升模型预测的准确性和泛化能力。 文件目录123456789101112131415├─data│ ├─input│ └─output│ ├─corpus│ ├─feature│ │ ├─amt│ │ ├─dbowd2v│ │ ├─dmd2v│ │ ├─tfidf│ │ └─w2v│ ├─model│ └─result│ ├─sub│ └─val└─src 文件目录如上所示。data 目录中包含所有的数据文件，由于数据文件较大，我没有上传；src 目录中包含所有的代码文件。以下是详细介绍： data/input：所有的源文件和停用词文件 data/output/corpus：数据预处理后的数据文件 data/output/feature：生成的各类特征文件 data/output/model：训练 Doc2Vec 和 Word2Vec 模型时产生的模型文件 data/result/sub：生成的结果文件（可提交至线上） data/result/val：交叉验证产生的结果文件（这里的文件不提交） 目录内的文件大致如下： 执行顺序我将交叉验证和全量训练预测分成了两个文件：ai_judge_cv.ipynb 和 ai_judge.ipynb。其次，还有一些冗余的脚本文件，分别是位于 src 目录下的 python 脚本。这些脚本文件只是 ai_judge_cv.ipynb 中的冗余代码，只是整理成了 python 脚本。脚本文件的执行顺序如下： pre.py：分词和去除停用词 feat_tfidf_stack.py：构造 tf-idf stacking 特征 feat_amount.py：构造统计特征 train_d2v_model.py：训练 Doc2Vec 模型 feat_dbow_stack.py：构造 Doc2Vec-DBOW stacking 特征 feat_dm_stack.py：构造 Doc2Vec-DM stacking 特征 train_w2v_model.py：训练 Word2Vec 模型 feat_w2v.py：构造 Word2Vec 特征 xgb_ens.py：使用 XGBoost 结合所有特征进行交叉验证 强烈推荐直接使用那两个 notebook 文件，因为这些冗余脚本有可能在我更新了 notebook 文件后并没有及时更新。 展望 在数据预处理阶段，我直接使用的是 1-gram 词作为语料库。在实际中，char 可能包含一些粒度更小的意义，n-gram 词可能可以考虑到词组语义。在数据挖掘中，数据也是一个重要的方面。因此，将这两种方式也考虑进来，做一定的组合尝试，可能可以达到更好的表达效果，从而提升预测精度 在特征构建阶段，实际上还有很多的特征没有挖掘，其中包括：基于 LDA 的特征、基于 LSI 的特征等 在模型部分，我没有尝试目前最热门的深度学习，这也算是我的一大遗憾（主要考虑到我的实验室计算机没有 GPU，惨兮兮）。目前很多的自然语言处理的数据挖掘比赛，前几名都或多或少的结合了深度学习，这不是一个偶然现象，能够说明在这个领域，深度学习能够很好的契合，并且实现不错的效果。另外模型融合的方式也只考虑了 stacking，其他的融合方式（例如：bagging、blending 等） 也没有做出尝试 嘿！感谢以下朋友，他们指出了我在这个项目中出现的纰漏： @开心老黄，他指出对于统计特征中的金额特征，我只提取了阿拉伯数字，而实际案件文本中还包含一些中文和繁体字的金额（例如：一千万元、贰佰元等），这类特征提取完备，效果能更好 如果您有任何的想法，例如：发现某处有 bug、觉得我对某个方法的讲解不正确或者不透彻、有更加有创意的见解，欢迎随时发 issue 或者 pull request 或者直接与我讨论！另外您若能 star 或者 fork 这个项目以激励刚刚踏入数据挖掘的我，我会感激不尽~]]></content>
      <categories>
        <category>数据挖掘比赛</category>
      </categories>
  </entry>
</search>
