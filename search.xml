<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【剑指Offer】面试题44：数字序列中某一位的数字]]></title>
    <url>%2F2018%2F02%2F22%2F%E3%80%90%E5%89%91%E6%8C%87Offer%E3%80%91%E9%9D%A2%E8%AF%95%E9%A2%9844%EF%BC%9A%E6%95%B0%E5%AD%97%E5%BA%8F%E5%88%97%E4%B8%AD%E6%9F%90%E4%B8%80%E4%BD%8D%E7%9A%84%E6%95%B0%E5%AD%97%2F</url>
    <content type="text"><![CDATA[题目：数字以 012345678912131415… 的格式序列化到一个字符序列中。在这个序列中，第 5 位（从 0 开始计数）是 5，第 13 位是 1，第 19 位是 4，等等。请写一个函数，求任意第 n 位对应的数字。 思路 1：穷举最直接的思路是：从 0 开始逐个枚举数字，累加数字的位数：当位数小于 n 时，继续枚举数字；当位数等于或大于 n 时，则说明该数字中的某一位就是第 n 位对应的数字，然后找出其中所对应的那一位数字。 这个算法是极为简单的思路，时间复杂度可能达不到面试官的要求，我们还需要思考更快的算法。 思路 2：分析例子找规律我们一定要从 0 开始一个一个枚举数字吗？不一定。在到我们想要的数字之前，很多数字都是可以忽略的。基于这个想法，我们来分析具体的例子，看看如何忽略大部分不需要枚举的数字。为了说清楚这个算法，我们取 n=1001。因此，我们需要找出第 1001 位对应的数字。 0~9 这 10 个数字是一位数，$1001-10=991 \geq 0$，因此我们可以忽略这 10 个数字。我们从其后搜索第 991 位 10~99 这 90 个数字是二位数，$991-90*2=811 \geq 0$，因此我们也可以忽略这 90 个数字。我们从其后搜索第 811 位 100~999 这 900 个数字是三位数，$811-900*3=-1889&lt;0$，因此我们要找的数字肯定是出现在 100~999 中。 由于 $811=270*3+1$，因此我们要找的数字是在 100~999 中的第 270 个（从 0 开始计数）数的第 1 位（从 0 开始计数），所以 1001 位对应的数字是 7（370 中的中间一位） 根据以上例子的分析，我们可以将思路进行总结：首先找出是几位数，然后找出是哪个数，最后找出是这个数中的第几位。这个算法的时间复杂度小于 $O(log\,n)$，明显快于上一种算法。]]></content>
      <categories>
        <category>剑指Offer</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【剑指Offer】面试题43：1~n整数中1出现的次数]]></title>
    <url>%2F2018%2F02%2F20%2F%E3%80%90%E5%89%91%E6%8C%87Offer%E3%80%91%E9%9D%A2%E8%AF%95%E9%A2%9843%EF%BC%9A1-n%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%2F</url>
    <content type="text"><![CDATA[题目：输入一个整数 n，求 1~n 这 n 个整数的十进制表示中 1 出现的次数。例如，输入 12，1~12 这些整数中包含 1 的数字有 1、10、11 和 12，1 一共出现了 5 次。 思路 1：穷举最直接的思路就是：遍历 1~n，找出每个数字中 1 出现的次数。如何找出某个数字中 1 出现的次数呢？对 10 求余，判断数字个位数是否为一，如果结果等于 1 则数字的个位数为 1，否则数字的个位数不为 1；如果这个数字大于 10，则除以 10 之后再进行求余操作。 我们来分析这个算法的时间复杂度，n 这个数字的十进制表示的长度为 $log_{10 }n$，因此求余除法操作的时间复杂度是 O(logn)，我们需要遍历 n 次，因此整个算法的时间复杂度是 O(nlogn)。这样的时间复杂度比较高，我们需要思考更快的算法。 思路 2：排列我们把 n 设置的大一些，让 n=21345，以便于之后的分析。为了更好的阐述这个算法，我们先来就具体的例子来分析一遍。 我们首先把分析对象分成两部分，1~21345 分成 1~19999 和 20000~21345 先对第一部分 1~19999 进行分析。计算 1 出现的次数实际上可以理解为十进制表示中某一位为 1 的排列的个数。因此，1~19999 中，当万位为 1 时其余 4 位为任意数字的排列的个数为 $10*10*10*10=10^4=10000$；当千位为 1 时，万位只能取 0 或 1，其他位取任意数字，的排列的个数为 $2*10*10*10=2000$；同理，百位为 1、十位为 1、个位为 1 的排列次数都是 $2*10*10*10=2000$。因此，1~19999 中，总的排列的个数为 $10000+2000*4=18000$ 然后对第二部分 20000~21345 进行分析，因为我们将 1 出现的次数理解成十进制表示中某一位为 1 的排列的个数，所以 20000~21345 可以转换成 0~1345。于是这个部分就变成了求解 1~1345（0 去掉不影响）中 1 出现的次数 对于 1~1345 我们可以根据步骤 1 和 2 递归的求解。把 1~1345 分成 1~999 和 1000~1345。效仿于第 2 步骤的分析，1~999 中 1 出现的次数为 $10*10*10=1000$。1000~1345 可以转换成 1~345 加上 345 次 对于 1~345 我们可以根据步骤 1 和 2 递归的求解。把 1~345 分成 1~299 和 300~345。效仿于第 2 步骤的分析，1~299 中 1 出现的次数为 $10*10+2*3*10=260$。300~345 可以转换成 1~45 对于 1~45 我们可以根据步骤 1 和 2 递归的求解。把 1~45 分成 1~39 和 40~45。效仿于第 2 步骤的分析，1~39 中 1 出现的次数为 $10+4=14$。40~45 可以转换成 1~5 1~5 中 1 出现的次数为 1 因此，1~21345 中 1 出现的次数为 $18000+1000+345+260+14+1=19620$ 根据以上的分析，我们将这个算法进行总结：计算 1 出现的次数实际上可以理解为十进制表示中某一位为 1 的排列的个数，因此我们可以使用递归的方法，将 n 从十进制表示的最高位到最低位一步一步计算 1 出现的次数。这个算法的递归深度是 $log\,n$，其他的操作都是常数级别的，因此总的时间复杂度为 $O(log\,n)$。]]></content>
      <categories>
        <category>剑指Offer</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【剑指Offer】面试题42：连续子数组的最大和]]></title>
    <url>%2F2018%2F02%2F18%2F%E3%80%90%E5%89%91%E6%8C%87Offer%E3%80%91%E9%9D%A2%E8%AF%95%E9%A2%9842%EF%BC%9A%E8%BF%9E%E7%BB%AD%E5%AD%90%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%A4%A7%E5%92%8C%2F</url>
    <content type="text"><![CDATA[题目：输入一个整型数组，数组里有正数也有负数。数组中的一个或连续多个整数组成一个子数组。求所有子数组的和的最大值。要求时间复杂度为 O(n)。 例如，输入的数组为 {1, -2, 3, 10, -4, 7, 2, -5}，和最大的子数组为 {3, 10, -4, 7, 2}，因此输出为该子数组的和 18。 思路 1：穷举最直接的思路就是：穷举数组的所有子数组的情况，然后计算它们的和，找出最大的值。子数组的数量为 $\frac{n(n - 1)}{2}$，因此时间复杂度为 $O(n^2)$，这个时间复杂度是不能让人接受的，我们还得想想其他办法。 思路 2：举例分析数组的规律当我们没有什么思路时，我们可以举例来分析一下数组的规律。我们尝试从头到尾的逐个累加数组中的数字。初始化和为 0。 累加第一个数字 1。和为 1 累加第二个数字 -2。和为 -1，小于 0 累加第三个数字 3。和为 2。这个和比第三个数字的值还小，我们舍弃之前的子数组，从当前数字开始重新累加。这时，和为3 从以上分析中，我们可以发现，一旦累加和小于 0 之后，以这个数为结尾的子数组对于求解和最大的子数组就没什么用了，这是应该舍弃，然后从将和归零，重新累加寻找和最大的子数组。 累加第四个数字 10。和为 13 累加第五个数字 -4。和为 9 累加第六个数字 7。和为 16 累加第七个数字 2。和为 18 累加第八个数字 -5。和为 13 每次累加一个数字其实就相当于在子数组中增加一个数字，计算和也就是计算这个子数组的和。从以上的分析中，我们可以发现，当累加到第七个数字时，和最大，此时的子数组为 {3, 10, -4, 7, 2}，该子数组的和 18 就是我们要找的最大和。 因此，我们可以将思路总结一下：从头到尾的累加数组中的数字，当累加和小于 0 时，就舍弃此时的累加并重置和为当前数字；使用一个临时变量记录累加过程中计算的和的最大值。 该算法只需要从头到尾的遍历一遍数组即可，因此时间复杂度是 O(n)。 思路 3：动态规划一旦涉及到求最值的问题，一般都可以用动态规划来解。这道题希望求解最大值，并且求解的是数组的子数组的相关问题，可能会有重叠的子问题，可以使用动态规划来解。 我们使用 f(i) 来表示以数组中第 i 个数字结尾的子数组的最大和，因此我们需要求解数组中以每个数字结尾的子数组的最大和的最大值，也就是 $max \, f(i), \, 0 \leq i \lt n$。状态转移方程满足： $$f(i) =\begin{cases}pData[i], &amp; i=0 \lor f(i-1) \leq 0\\f(i-1) + pData[i], &amp; i \neq 0 \land f(i-1)&gt;0\end{cases}$$ 这个公式表示：如果第 i-1 个数字结尾的子数组的和小于 0，那么第 i 个数字结尾的子数组就舍弃之前的数字，只包含第 i 个数字，此时的和为第 i 个数字本身；否则，“第 i 个数字结尾的子数组的和” = “第 i-1 个数字结尾的子数组的和” + “第 i 个数字”。 实际上，你仔细想想，思路 3 和思路 2 在本质上是相同的，只是表达方式不一样。这个思路的时间复杂度同样是 O(n)。]]></content>
      <categories>
        <category>剑指Offer</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【剑指Offer】面试题41：数据流中的中位数]]></title>
    <url>%2F2018%2F02%2F17%2F%E3%80%90%E5%89%91%E6%8C%87Offer%E3%80%91%E9%9D%A2%E8%AF%95%E9%A2%9841%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%AD%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0%2F</url>
    <content type="text"><![CDATA[题目：如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。 思路 1：没有排序的数组由于数据是从数据流中读出来的，因此数据的个数随着时间的增长而增多，我们需要使用一个数据容器去接收来自数据流中的数据。 最直接的思路是：使用不排序的数组作为数据容器。不排序的意思是，每当从数据流中读取一个数据时，直接将这个数据插入数组的尾部，不做任何操作。这样当我们需要计算数组中的中位数时，我们便可以使用 Partition 的思想，找出数组中的中位数。 使用不排序的数组作为数据容器，插入数据的时间复杂度是 O(1)，找出中位数的时间复杂度是 O(n)。 思路 2：排序的数组这个算法的思路是：使用排序的数组作为数据容器。排序的意思是，每当从数据流中读取一个数据时，插入数组后，数组都要保持排序的状态，也就是说，每次插入数据都需要把比它大的数值往后移动一个位置。在排序完的数组中找出中位数，只需要先判断数组中的元素个数是奇数还是偶数，如果是奇数，那么取出索引为 $\frac{n}{2}$ 的元素即可，否则取出索引为 $\frac{n - 1}{2}$ 和 $\frac{n}{2}$ 的元素加权平均即可。 使用排序的数组作为数据容器，插入数据的时间复杂度是 O(n)，找出中位数的时间复杂度是 O(1)。 思路 3：排序的链表这个算法的思路是：使用排序的链表作为数据容器。当从数据流中读取一个数据时，从表头遍历到表尾，插入新数据保持链表排序。在插入新数据的同时，定义两个指针，分别指向链表中间的节点，当节点个数为奇数时，两个指针指向同一个节点。 使用排序的链表作为数据容器，插入数据的时间复杂度是 O(n)，找出中位数的时间复杂度是 O(1)。 思路 4：二叉搜索树这个算法的思路是：使用二叉搜索树作为数据容器。二叉搜索树是排序好的，而且插入新数据时，平均时间复杂度是 O(logn)。但当二叉搜索树极度不平衡时，二叉搜索树与排序的链表是相似的，插入数据的时间复杂度也提升至 O(n)。 使用二叉搜索树作为数据容器，插入数据的平均时间复杂度是 O(logn)，最差情况的时间复杂度是 O(n)，找出中位数的时间复杂度是 O(logn)，最差情况的时间复杂度是 O(n)。 思路 5：AVL 树和红黑树这个算法的思路是：使用平衡的二叉搜索树（AVL 树或者红黑树）作为数据容器。AVL 树和红黑树都在某种程度上保证了二叉搜索树的左右子树在深度上差异不大，从而使得整棵二叉搜索树平衡。 使用 AVL 树和红黑树作为数据容器，插入数据的时间复杂度是 O(logn)，找出中位数的时间复杂度是 O(1)。 思路 6：最大堆和最小堆之前的思路是希望对从数据流中读出的数据进行从头到尾彻底的排序，然后找出中位数。我们能不能找到一个办法不需要进行从头到尾的彻底的排序呢？当然可以。我们的目的是希望找到中位数，也就是说，比中位数小的数和比中位数大的数并不一定需要排序，只用确保这些数比中位数小或者大就行。 因此，这个算法的思路是：使用最大堆作为比中位数小的数的数据容器，使用最小堆作为中位数大的数的数据容器。最大堆的堆顶是这个数据容器中的最大值，最小堆的堆顶是这个数据容器中的最小值。这时，我们只需要保证两个数据容器的元素个数相差不超过 1，并且最大堆的堆顶小于最小堆的堆顶。 使用最大堆和最小堆作为数据容器，插入数据的时间复杂度是 O(logn)，找出中位数的时间复杂度是 O(1)。]]></content>
      <categories>
        <category>剑指Offer</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【剑指Offer】面试题40：最小的k个数]]></title>
    <url>%2F2018%2F02%2F16%2F%E3%80%90%E5%89%91%E6%8C%87Offer%E3%80%91%E9%9D%A2%E8%AF%95%E9%A2%9840%EF%BC%9A%E6%9C%80%E5%B0%8F%E7%9A%84k%E4%B8%AA%E6%95%B0%2F</url>
    <content type="text"><![CDATA[题目：输入 n 个整数，找出其中最小的 k 个数。例如，输入 4、5、1、6、2、7、3、8 这 8 个数字，则最小的 4 个数字是 1、2、3、4。 思路 1：排序最直接的思路是：将这个输入的数组使用排序算法排序，然后前 k 个数字就是最小的 k 个数字。但是这种方法的时间复杂度是 O(nlogn)，比较高，而且需要修改输入的数组，并且不适用于海量数据（海量数据无法一次性读入内存中）。 思路 2：Partition我们一定要将整个数组彻底的排序才能找到最小的 k 个数字吗？不一定，这个算法的思路灵感来源于快速排序。 思路是： 挑选数组中的一个数字，调整数组的顺序，使得比选中的数字小的数字排在它的左边，比选中的数字大的数字排在它的右边 查看这时选中的数字的索引是否等于 k 或者 k+1。如果等于 k，那么这时选中的数字与它左边的数字就是最小的 k 个数；如果等于 k+1，那么这时选中的数字的左边的数字就是最小的 k 个数；否则继续对右边的部分进行递归的搜索 这个算法不需要将整个数据进行彻底的排序，只需要局部的排序就能达到目的。时间复杂度为 O(n)，但是需要修改输入的数组，并且不适用于海量数据。 思路 3：堆和红黑树这个算法的思路其实很自然，就是使用一个数据容器装着最小的 k 个数。具体是： 从头到尾遍历数组 如果数据容器没有装满，就将数字装入数据容器中；如果数据容器装满了，就选出数据容器中最大的数字，拿当前数字与这个最大的数字进行比较，如果当前数字大则抛弃它，如果当前数字小则替换掉最大的数字 当数据容器选择堆或者红黑树时，对数据容器中的元素进行增删查只需要 O(logk) 的时间复杂度，然后我们需要遍历整个数组，因此这个算法的时间复杂度是 O(nlogk)。这个算法不需要修改输入数组，而且可以使用流的方式处理海量数据。]]></content>
      <categories>
        <category>剑指Offer</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【剑指Offer】面试题39：数组中出现次数超过一半的数字]]></title>
    <url>%2F2018%2F02%2F15%2F%E3%80%90%E5%89%91%E6%8C%87Offer%E3%80%91%E9%9D%A2%E8%AF%95%E9%A2%9839%EF%BC%9A%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0%E8%B6%85%E8%BF%87%E4%B8%80%E5%8D%8A%E7%9A%84%E6%95%B0%E5%AD%97%2F</url>
    <content type="text"><![CDATA[题目：数组中有一个数字出现的次数超过了数组长度的一半，请找出这个数字。例如，输入一个长度为 9 的数组 {1, 2, 3, 2, 2, 2, 5, 4, 2}。由于数组 2 在数组中出现了 5 次，超出数组长度的一半，因此输出 2。 思路 1：排序最直接的思路就是：将这个数组排序，排序完之后数组就是有序的了，然后直接访问这个数组的中位数，也就是索引为 $\frac{n}{2}$ 的元素，就是我们寻找的数字。但是这个算法的时间复杂度比较高，排序所需要的时间复杂度是 O(nlogn)，并且需要修改输入的数组。这个算法能够在面试官刚提出题目之后快速的答上来，可以体现自己思维比较敏捷，但是需要能够提出时间复杂度更小的算法才能够获得面试官的青睐。 思路 2：Partition从上一个思路，我们可以得到启发，实际上我们就是要找到排好序之后索引为 $\frac{n}{2}$ 的元素，也就是中位数。那我们一定要把整个数组完全的排好序吗？这不一定，这个算法的灵感来源于快速排序。 算法思路就是： 我们先选择数组中的一个数字，然后调整数组中数字的顺序，使得比选中的数字小的数字放置于它的左边，比选中的数字大的数字放置于它的右边 判断选中的数字的索引是否等于 $\frac{n}{2}$。如果相等，这个数字就是我们寻找的数字，也就是中位数；如果大于 $\frac{n}{2}$，那么中位数在它的左边，对左边的部分递归的查找；如果小于 $\frac{n}{2}$，那么中位数在它的右边，对右边的部分递归的查找 这样做不需要对整个数组排序，只需要局部的排序就能够达到目的，算法的时间复杂度是 O(n)。但是缺点在于依然需要修改输入的数组。 思路 3：临时变量输入的数组中有一个数字超过了数组长度的一半，也就是说，这个数字的个数比其他所有数字的个数都要多。基于这个特点，我们便产生了一个不需要修改数组的思路。 思路是：我们使用一个临时变量存储数字，另一个临时变量存储次数。从头到尾的遍历数组，如果当前数字与数字变量（也就是之前保存的数字）中的数字相同，次数变量就加 1，否则就减 1；如果次数为 0，那么我们需要将数字变量赋值为当前数字，并把次数变量赋值为 1。由于我们要寻找的数字比其他数字的个数都要多，因此最后在数字变量中存储并且次数变量大于 0 的数字就是我们要寻找的数字。 这个算法只需要从头到尾的遍历一遍数组，因此算法的时间复杂度是 O(n)，并且不需要修改输入的数组。]]></content>
      <categories>
        <category>剑指Offer</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【数据挖掘比赛】企业经营退出风险预测]]></title>
    <url>%2F2018%2F02%2F07%2F%E3%80%90%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%E3%80%91%E4%BC%81%E4%B8%9A%E7%BB%8F%E8%90%A5%E9%80%80%E5%87%BA%E9%A3%8E%E9%99%A9%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[这是我的第一个数据挖掘比赛，CCF 大数据与计算智能大赛（BDCI）中的一题：企业经营退出风险预测。最终取得复赛 A 榜第 3，B 榜第 9 (Top 1.58%) 的成绩。 这个比赛 12 月中旬就结束了，硬是被我拖到现在才来总结，我这拖延症真的是……现在回忆起这个比赛，比赛时的那种郁闷感依然记忆犹新。我在复赛的第 5 天便达到了分数 6924，但之后一直无法提分，这种烦躁感当时给我带来了挺大的困扰（当然最后还是提升到了分数 6930）。等比赛结束之后，我回过头来看，其实当时我参赛的心态是不端正的，功利心太强，这样带来的问题就是比赛心态的爆炸，自己的眼界会被约束，提分方式的想象力也会被限制。最好的心态应该是抱着学习的心态参赛，只要能够学到一点点新的东西，就会感到惊喜。 另外一个想说的点是，我们团队在复赛 A 榜中排名第 3，但是切换 B 榜之后，便跌到第 9 了，这个现象直接导致我们团队没有进入决赛，因此我会在后文中谈一谈为什么会有这个现象。 我的另一位队友 JinjinLin 也开源了解决方案，详情请见 JinjinLin 的解决方案 项目源码：https://github.com/ShawnyXiao/2017-CCF-BDCI-Enterprise Why?CCF 举办的这次大赛中这么多比赛，为什么唯独选择这个呢？ 因为门槛低。我在参赛之前对所有的比赛有过大致的了解，其中比赛类型包括：自然语言处理（NLP）、计算机视觉（CV）和传统的数据挖掘比赛等等。作为一个第一次参赛的新人，我的重心不会放在需要一定的门槛的比赛，因此就排除了 NLP 和 CV 的比赛，再挑一个门槛最低的，那么目标就锁定了，于是我便将重心放在了企业经营退出风险预测这个比赛。 因为有师兄带（提供 baseline，指导尝试方向）。今年的 CCF 举办的大赛，我们实验室不少人参赛了，其中也包括不少往年拿过奖的师兄，他们有参赛经验。作为一只菜鸟，自然是希望有人能够给予少走弯路的建议。而师兄也建议新手参加这个方式相对简单的比赛作为入门。 为什么我想要说一下这个呢，因为我相信未来有很多的新人会尝试加入数据挖掘的阵营中，他们也会遇到相同的境遇，我希望能够将我当时的一些思考与选择作为他们的参考选项，以便于他们做出他们的最优选择。 代码框架第一次参赛，可以说连 Python 的语法都不熟悉，更何况 pandas 的各种操作。这时候师兄给的 baseline 就显得十分重要了。当中的各种基础操作，例如：文件读取、数据定义、分组聚集等等，对我来说都是新鲜的。其中最为关键的是传统的数据挖掘比赛中的代码框架。我们来看一下，这个极为经典的代码框架（非原始 baseline 框架，我做了一些修改）。 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 1. 导入库import numpy as npimport pandas as pd...# 2. 读取数据文件train = pd.read_csv('../data/input/train.csv')test = pd.read_csv('../data/input/evaluation_public.csv')...# 3. 定义特征构建函数def get_entbase_feature(df): ...def get_alter_feature(df): ......# 4. 调用函数，构建特征entbase_feat = get_entbase_feature(entbase)alter_feat = get_alter_feature(alter)...# 5. 拆分数据集的特征与标签dataset = pd.merge(entbase_feat, alter_feat, on='EID', how='left')...trainset = pd.merge(train, dataset, on='EID', how='left')testset = pd.merge(test, dataset, on='EID', how='left')train_feature = trainset.drop(['TARGET', 'ENDDATE'], axis=1)train_label = trainset.TARGET.valuestest_feature = testsettest_index = testset.EID.values# 6. 模型的交叉验证...iterations, best_score = xgb_cv(train_feature, train_label, params, config['folds'], config['rounds'])...# 7. 模型的训练与预测...model, pred = xgb_predict(train_feature, train_label, test_feature, iterations, params)...# 8. 结果文件的写出res = store_result(test_index, pred, 0.18, '1207-xgb-%f(r%d)' % (best_score, iterations)) 从上面给的样例代码中，我们可以观察到整个代码的框架如下： 导入库 读取数据文件 定义特征构建函数 调用函数，构建特征 拆分数据集的特征与标签 模型的交叉验证 模型的训练与预测 结果文件的写出 使用这样一个代码框架，能够十分清晰的知道整个数据挖掘的流程，这一点对于第一次参赛的信任是尤为重要的。另外当我们想要提分时，我们只需要在特定的部分做出相应的修改就能够达到目的。例如：我希望构建新的特征，来提升我的分数，那么这时只需要新增框架中的第 3 和第 4 部分即可。 数据预处理这个数据集中存在着不少的脏数据，这个阶段便是对这些脏数据进行处理，其中包括： 转化或者移除数据中存在的中文字符 针对性的空值填充 针对性地去除重复值 异常值的处理（这点我没有做） 特征我将特征分为 5 个部分，分别是基础特征、偏离值特征、交叉特征和想象力特征。 1. 基础特征基础特征是比赛中最容易想到的特征，其中包括： 保留字段。数据集中某些关键字段直接保留成特征，例如：uid、ZCZB、RGYEAR、INUM、ENUM 等 统计特征。以某几个字段作为分组字段，然后进行统计操作，统计操作包括：计数、求和、最小值、最大值、最小最大差值、均值、标准差、比例等 特定集合中的统计特征。先进行过滤，然后以某几个字段作为分组字段，然后进行统计操作。例如：统计近 1、2、5 年内的修改数额的最小值、最大值和均值等 2. 偏离值特征偏离值特征指单个个体与分组之间的偏离距离。以下的代码所生成的特征便是这一类特征： 1234567891011121314151617181920dataset['MPNUM_CLASS'] = dataset['INUM'].apply(lambda x : x if x &lt;= 4 else 5)dataset['FSTINUM_CLASS'] = dataset['FSTINUM'].apply(lambda x : x if x &lt;= 6 else 7)dataset.fillna(value=&#123;'alt_count': 0, 'rig_count': 0&#125;, inplace=True)for column in ['MPNUM', 'INUM', 'FINZB', 'FSTINUM', 'TZINUM', 'ENUM', 'ZCZB', 'allnum', 'RGYEAR', 'alt_count', 'rig_count']: groupby_list = [['HY'], ['ETYPE'], ['HY', 'ETYPE'], ['HY', 'PROV'], ['ETYPE', 'PROV'], ['MPNUM_CLASS'], ['FSTINUM_CLASS']] for groupby in groupby_list: if 'MPNUM_CLASS' in groupby and column == 'MPNUM': continue if 'FSTINUM_CLASS' in groupby and column == 'FSTINUM': continue groupby_keylist = [] for key in groupby: groupby_keylist.append(dataset[key]) tmp = dataset[column].groupby(groupby_keylist).agg([sum, min, max, np.mean]).reset_index() tmp = pd.merge(dataset, tmp, on=groupby, how='left') dataset['ent_' + column.lower() + '-mean_gb_' + '_'.join(groupby).lower()] = dataset[column] - tmp['mean'] dataset['ent_' + column.lower() + '-min_gb_' + '_'.join(groupby).lower()] = dataset[column] - tmp['min'] dataset['ent_' + column.lower() + '-max_gb_' + '_'.join(groupby).lower()] = dataset[column] - tmp['max'] dataset['ent_' + column.lower() + '/sum_gb_' + '_'.join(groupby).lower()] = dataset[column] / tmp['sum']dataset.drop(['MPNUM_CLASS', 'FSTINUM_CLASS'], axis=1, inplace=True) 这段代码的意思是： 首先，根据分组字段对数据集进行分组 然后计算每个个体与分组的均值、最小值、最大值和求和值之间的偏离距离 这类特征对于这个比赛十分有效，是我分数大幅上升的一个原因。 3. 交叉特征交叉特征指不单单从一个角度去构建特征，而从多个角度构建够特征，或者说将特征之间相互作用后生成新的特征。这类特征包括： 加减乘除特征。将特征与特征做加减乘除操作，也就是所谓的暴力出奇迹。例如：MPNUM+INUM、FINZB/ZCZB 等 独热交叉特征。将一些特征做独热编码后，然后乘以某个特征。例如：将 HY 做独热编码后，乘以 ZCZB、RGYEAR 等 多项式交叉特征。对特征做多项式组合。例如：MPNUM^2+INUM 等（我没有做这类交叉特征） 交叉特征的效果也十分明显，能显著的提升分数，其中独热交叉特征在这个比赛中最为有效。 4. 想象力特征想象力特征这个词是我自己构造的，指的是根据实际的业务场景，思考其中可能存在的一些隐晦的特征。例如：投资表中，就可以构建一个投资网络，然后基于这个网络提取相关的特征。这个思路来自我的师兄 @Kaho，这也是我赛后才了解到的特征构造方式，十分新颖。 模型模型部分包括：单模型的提分与多模型融合。 首先，谈谈单模型的提分。在这个比赛中，根据师兄的建议，我选择了 XGBoost，使用它的原因在于： 树模型有较强的可解释性，往往简单且高效 树模型对于异常值有较强的鲁棒性 树模型对特征处理的要求比较低，不需要对特征进行归一化与空值填充 其次，是多模型融合。这部分是我的另一位队友做的，因此我没有过多的尝试多模型融合。在这个比赛中，我们团队的融合效果不是太好，加权融合之后分数仅提升 1 至 2 个千。 踩过的坑新人入赛不踩坑是不可能的，比赛中我是踩了无数个坑，其中比较有意思的，比较隐晦的有这么几个： 不要带着刻板印象去筛选特征，换句话说，你不要觉得其他比赛没用的特征对于这个比赛同样没用。在这个比赛中，ID 特征是一个强特征，我刚开始就带着刻板印象把它删了，导致 3 个千分点的劣势，发现这个问题也耗费了不少时间 在对 dataframe 排序之后一定要 调用 reset_index(drop=True)，不然之后对这个 dataframe 的各种操作的是误操作。这个坑同样耗费了我不少的精力 不要太早就开始模型调参，模型调参只能带来极少的提升，在你的分数没有达到一定竞争力的时候，调参带来的收益是极少的，因此在调参这个举动的价值在比赛早期是较低的 复赛开始后，初赛数据别果断抛弃，应该试一试效果，辩证式的采纳 没尝试的点 没尝试融合大法。因为团队中有队员负责融合，所以在比赛中我没有尝试融合大法，这点比较可惜。另外我们团队的融合策略是 blending（加权融合），还可以尝试的策略包括：stacking、bagging 等 没尝试使用初赛的数据。这点输在新人没经验，根本没有意识到可以使用初赛的数据 未进决赛的原因分析我们团队在复赛 A 榜中排名第 3，但是切换 B 榜之后，便跌到第 9 了，这个现象直接导致我们团队没有进入决赛，在赛后我进行了认真的分析与思考，并且与他人探讨，大致总结了几点原因： 未使用初赛提供的数据。由于我们是新人队伍，使用初赛数据这个套路我们完全没有考虑到，这样就使得其他既使用了复赛数据也使用了初赛数据的队伍能够占据较大优势 我们加权融合的依据是 A 榜的线上分数，这样有极大记录过拟合 A 榜，更好的做法应该是综合考虑线下分数与 A 榜线上分数，以避免出现过拟合现象 我们队伍都是来自一个实验室，和队之后，队伍内部有比较多的交流，这可能导致我们的特征相似度比较大，这样融合之后的效果不会特别好，因此我们融合值提升了 1 至 2 个千分点 嘿！感谢以下朋友，他们向我输送了一些新的观点： @/微笑/:)/wx，他提出：我们团队来自一个实验室，特征可能比较相似，导致融合效果不好 如果您有任何的想法，例如：发现某处有 bug、觉得我对某个方法的讲解不正确或者不透彻、有更加有创意的见解，欢迎随时发 issue 或者 pull request 或者直接与我讨论！另外您若能 star 或者 fork 这个项目以激励刚刚踏入数据挖掘的我，我会感激不尽~]]></content>
      <categories>
        <category>数据挖掘比赛</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【数据挖掘比赛】让AI当法官]]></title>
    <url>%2F2018%2F01%2F17%2F%E3%80%90%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%E3%80%91%E8%AE%A9AI%E5%BD%93%E6%B3%95%E5%AE%98%2F</url>
    <content type="text"><![CDATA[这是我近期参加的一个数据挖掘比赛，CCF 大数据与计算智能大赛（BDCI）中的一题：让 AI 当法官。但是由于时间冲突与一些个人原因，我只参与并完成了初赛任务（罚金类别预测），并未完成复赛任务（法律条款预测）。在初赛成绩中，取得 A 榜第 5、B 榜第 7 (Top 1.68%) 的成绩（这个成绩实际上只用了 if-idf 特征和 Word2Vec 特征，该项目在初赛结束后做了不少尝试和改进，效果应该会更优于初赛）。 该项目是我的第一个有关文本分类的项目，所以在做这题之前，我没有任何自然语言处理（NLP）的知识积累。因此，通过参与这个比赛，我的初衷是希望学到一些自然语言处理的基础知识，所以名次对于我来说没有那么重要了。 作为一个零基础的选手，自己从零构建整个项目是非常困难的。于是，我搜索了往年的有关文本分类的比赛，挑出了一些对于我个人而言，比较容易阅读的一些方案与源码。我选择了 2016 年的比赛：大数据精准营销中搜狗用户画像挖掘，并找到了一等奖与二等奖的方案与源码，进行了详细的阅读，阅读源码的过程是很痛苦的，因为太多的这是什么那是什么这也不懂哪也不懂，但是正是经历过这样一个过程才能真正学到一些知识并完成一些实践。最终，依靠前辈们的方案与源码，零基础的我成功搭建了一个 baseline 项目并在其上做出自己的一些思考与优化。 项目源码：https://github.com/ShawnyXiao/2017-CCF-BDCI-AIJudge 方案1 数据预处理数据预处理包括分词和去除停用词，其达成的效果大致如下。 公诉机关霍邱县人民检察院。被告人许某甲，男，1975年9月20日生。2012年12月17日因涉嫌危险驾驶罪由霍邱县公安局取保候审。2013年3月4日经本院决定取保候审。霍邱县人民检察院以霍检刑诉（2013）42号起诉书指控被告人许某甲犯危险驾驶罪，于2013年2月27日向本院提起公诉。本院依法适用简易程序，实行独任审判，于2013年3月4日公开开庭审理了本案。霍邱县人民检察院检察员胡涛、被告人许某甲到庭参加诉讼。现已审理终结。霍邱县人民检察院指控：2012年12月2日19时30分左右，被告人许某甲酒后驾驶二轮摩托车沿霍寿路由南向北行驶至霍寿路与公园路交叉口时，与路边行人相撞，被公安民警查获。经六安市疾病预防控制中心鉴定，许某甲血液中乙醇含量为169.64mg／100ml。上述事实，被告人在开庭审理过程中亦无异议，并有被害人杨正响的陈述，证人李某甲的证言，《六安市疾病预防控制中心检验报告》六安市疾控交检字（2012）第155号，霍邱县公安局交通管理大队呼吸式酒精检测结果单，抽取当事人血样登记表，驾驶人信息查询结果单，道路交通事故赔偿调解协议书、经济赔偿凭证、谅解书，被告人的户籍信息等证据证明，足以认定。 会被转化为： 公诉 机关 霍邱县 人民检察院 被告人 许某 甲 男 1975 年 月 20 日生 2012 年 12 月 17 日 因涉嫌 危险 驾驶 罪 霍邱县 公安局 取保候审 2013 年 月 日经 本院 取保候审 霍邱县 人民检察院 以霍检 刑诉 2013 42 号 起诉书 指控 被告人 许某 甲犯 危险 驾驶 罪 2013 年 月 27 日向 本院 提起公诉 本院 依法 简易程序 实行 独任 审判 2013 年 月 日 公开 开庭审理 本案 霍邱县 人民检察院 检察员 胡涛 被告人 许某 甲 到庭 参加 诉讼 现已 审理 终结 霍邱县 人民检察院 指控 2012 年 12 月 日 19 时 30 分 被告人 许某 甲 酒后 驾驶 二轮 摩托车 沿霍寿路 由南向北 行驶 霍寿路 公园路 交叉口 时 路边 行人 相撞 公安民警 查获 六安市 疾病 预防 控制中心 鉴定 许某 甲 血液 中 乙醇 含量 169.64 mg 100ml 上述事实 被告人 开庭审理 过程 中 无异议 被害人 杨正响 陈述 证人 李某 甲 证言 六安市 疾病 预防 控制中心 检验 报告 六安市 疾控交 检字 2012 155 号 霍邱县 公安局 交通管理 大队 呼吸 式 酒精 检测 抽取 当事人 血样 登记表 驾驶 信息 查询 道路 交通事故 赔偿 调解 协议书 经济 赔偿 凭证 谅解 书 被告人 户籍 信息 证据 证明 足以认定 接下来，我会对这两个子处理进行介绍，并做出一些思考：在这些处理之上还能做些什么。 1.1 分词为了快速构建项目，我直接采取了比较热门的分词方案：结巴分词。 实际上，在此基础上还可以做的事情有很多（虽然我没有做），例如： 采用多种分词方案（例如：NLPIR、THULC 等），实现分词 使用某种模型（例如：贝叶斯模型等）比较这多种分词方案的效果 对分词后的语料库进行统计分析，归纳改语料库的特点（例如：字典长度为多少；低频词多吗；该不该过滤掉某些词等等） 1.2 去除停用词中文中有非常多的停用词，这些停用词对于我们的文本分类任务是无用的。因此，我采取的措施是：直接去除。 有些任务对于一些停用词是敏感的。在这个阶段，还可以做的有： 对停用词的进行分析，猜想某些停用词是有用的并验证猜想（个人感觉这个比赛的停用词是无用的） 2 特征构建我从 4 个方面对文本进行特征构建，分别是：基于 tf-idf 的特征、基于 Doc2Vec 的特征、基于 Word2Vec 的特征和统计特征。接下来，我会从这 4 个方面分别介绍。 2.1 基于 tf-idf 的特征tf-idf（英语：term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。tf-idf 是一种统计方法，用以评估一字词对于一个文档集或一个语料库中的其中一份文档的重要程度。字词的重要性随着它在文档中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。 我直接使用了 TfidfVectorizer 提取了语料库的 tf-idf 特征。但是 tf-idf 特征具有多维稀疏的特点。对于这类特征，直接扔给树模型的话不仅慢而且效果还差，因此比较流行的做法是做一层 stacking。我挑选了 LogisticRegression、BernoulliNB、MultinomialNB 和 LinearSVC 作为基模型分别对 tf-idf 特征进行训练，并构建下一层模型需要的特征，因此理论上能够产生 4*8 的特征列数。但最终根据实验结果，我移除掉了 LinearSVC 模型，因此只留下了 3*8 列的特征。 这里，还可以做的事情有： 选取更多的模型对 tf-idf 特征进行训练，并比较效果，选取实验效果最好的几个进行 stacking 2.2 基于 Doc2Vec 的特征我使用 Doc2Vec 方法，将文档直接表示成一个固定长度的向量。根据训练文档向量的网络结构的不同，可以分为 Distributed Memory（DM）与 Distributed Bag of Words（DBOW）两种模型。其中 DM 模型不仅考虑了词的上下文语义特征，还考虑到了词序信息。DBOW 模型则忽略了上下文词序信息，而专注于文档中的各个词的语义信息。我同时采用了 DBOW 和 DM 这两种模型构建文档向量，希望能够保留文档中完整的信息。 对于 Doc2Vec 模型，我选取的维数是 300。在训练文档向量的过程中，我发现增量训练似乎可以提升文本分类的精度，因此我在训练过程中增加了训练的次数，DBOW 模型的训练次数为 5，DM 模型的训练次数为 10。 同样的，我对于这两类文档向量分别做了一层 stacking，使用了一个简易的神经网络模型，只有一层 300 维的隐含层，进行训练并构建下一层模型需要的特征。 这个地方，还可以做得事情有： 比较 DBOW 和 DM 模型的效果，确定是否选用其中一种或者选用两种 选用多种模型（例如：LR、NN、KNN 等）对这两类文档向量进行效果对比，选取其中最好的几种模型进行 stacking 调优超参，包括 Doc2Vec 模型维数、增量训练次数等 2.3 基于 Word2Vec 的特征我使用了 Word2Vec 方法，将词语直接表示成一个固定长度的向量。对于 Word2Vec 模型，我选取的维数为 300，并将频数低于 5 的词语过滤掉。 那么对于一个文档来说，这些针对词语的向量要怎么处理呢？我选择了两种方式： 属于同一个文档的词向量，直接相加 属于同一个文档的词向量，加权平均（相加后的结果向量再除以文档的词数目） 由于一些个人原因，我只使用了第一种方式产生的特征向量。 实际上，这里可以做的事情还有： 对这两种方式生成的特征向量设计实验（例如：只选用其中一种特征向量，同时选用两种特征向量），进行效果比较，选取效果最好的方式 对于词向量维数和过滤的词频进行实验，选择最优的超参 设计步长参数，选取多个维数所产生的词向量（例如：选择步长为 50，维数从 100 到 500。那么可以产生 100、150、200、……、500 维数的词向量） 2.4 统计特征该文本分类任务是预测案件金额类别，因此案件文本中出现的金额是重要的。于是，我使用正则表达式匹配出案件文本中出现的所有金额，然后对同一个案件中出现的所有金额进行统计，包括：求和、最小值、最大值、最大最小差值、平均值、标准差。 这里还可以做的事情有： 统计案件文本的词的数目 利用案件中的一些关键词做特征，例如：酒驾、毒品等 案件文本中出现的日期 案件文本中出现的地点 3 模型 上图给出了本次项目的模型结构。我采用了模型融合中 stacking 的思想，使用两层的模型结构。第一层使用传统的机器学习模型 LogisticRegression、BernoulliNB 和 MultinomialNB，来训练 tf-idf 特征，从而学习案件文本中的用词特点；其次还使用神经网络模型来训练 Doc2Vec-DBOW 和 Doc2Vec-DM 生成的文档向量特征，从而学习案件文本中的词语的语义关联信息。第二层使用 XGBoost 模型，训练 Word2Vec、统计特征和第一层模型传来的概率特征，从而更深入的学习案件文本与金额类别之间的联系。采用 stacking 的模型融合思想，可以进一步的提升模型预测的准确性和泛化能力。 文件目录123456789101112131415├─data│ ├─input│ └─output│ ├─corpus│ ├─feature│ │ ├─amt│ │ ├─dbowd2v│ │ ├─dmd2v│ │ ├─tfidf│ │ └─w2v│ ├─model│ └─result│ ├─sub│ └─val└─src 文件目录如上所示。data 目录中包含所有的数据文件，由于数据文件较大，我没有上传；src 目录中包含所有的代码文件。以下是详细介绍： data/input：所有的源文件和停用词文件 data/output/corpus：数据预处理后的数据文件 data/output/feature：生成的各类特征文件 data/output/model：训练 Doc2Vec 和 Word2Vec 模型时产生的模型文件 data/result/sub：生成的结果文件（可提交至线上） data/result/val：交叉验证产生的结果文件（这里的文件不提交） 目录内的文件大致如下： 执行顺序我将交叉验证和全量训练预测分成了两个文件：ai_judge_cv.ipynb 和 ai_judge.ipynb。其次，还有一些冗余的脚本文件，分别是位于 src 目录下的 python 脚本。这些脚本文件只是 ai_judge_cv.ipynb 中的冗余代码，只是整理成了 python 脚本。脚本文件的执行顺序如下： pre.py：分词和去除停用词 feat_tfidf_stack.py：构造 tf-idf stacking 特征 feat_amount.py：构造统计特征 train_d2v_model.py：训练 Doc2Vec 模型 feat_dbow_stack.py：构造 Doc2Vec-DBOW stacking 特征 feat_dm_stack.py：构造 Doc2Vec-DM stacking 特征 train_w2v_model.py：训练 Word2Vec 模型 feat_w2v.py：构造 Word2Vec 特征 xgb_ens.py：使用 XGBoost 结合所有特征进行交叉验证 强烈推荐直接使用那两个 notebook 文件，因为这些冗余脚本有可能在我更新了 notebook 文件后并没有及时更新。 展望 在数据预处理阶段，我直接使用的是 1-gram 词作为语料库。在实际中，char 可能包含一些粒度更小的意义，n-gram 词可能可以考虑到词组语义。在数据挖掘中，数据也是一个重要的方面。因此，将这两种方式也考虑进来，做一定的组合尝试，可能可以达到更好的表达效果，从而提升预测精度 在特征构建阶段，实际上还有很多的特征没有挖掘，其中包括：基于 LDA 的特征、基于 LSI 的特征等 在模型部分，我没有尝试目前最热门的深度学习，这也算是我的一大遗憾（主要考虑到我的实验室计算机没有 GPU，惨兮兮）。目前很多的自然语言处理的数据挖掘比赛，前几名都或多或少的结合了深度学习，这不是一个偶然现象，能够说明在这个领域，深度学习能够很好的契合，并且实现不错的效果。另外模型融合的方式也只考虑了 stacking，其他的融合方式（例如：bagging、blending 等） 也没有做出尝试 嘿！感谢以下朋友，他们指出了我在这个项目中出现的纰漏： @开心老黄，他指出对于统计特征中的金额特征，我只提取了阿拉伯数字，而实际案件文本中还包含一些中文和繁体字的金额（例如：一千万元、贰佰元等），这类特征提取完备，效果能更好 如果您有任何的想法，例如：发现某处有 bug、觉得我对某个方法的讲解不正确或者不透彻、有更加有创意的见解，欢迎随时发 issue 或者 pull request 或者直接与我讨论！另外您若能 star 或者 fork 这个项目以激励刚刚踏入数据挖掘的我，我会感激不尽~]]></content>
      <categories>
        <category>数据挖掘比赛</category>
      </categories>
  </entry>
</search>
